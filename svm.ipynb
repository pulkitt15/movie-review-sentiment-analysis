{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvBAgpPEe00t"
      },
      "source": [
        "import requests,io\n",
        "from zipfile import ZipFile\n",
        "r = requests.get('https://github.com/pulkitt15/imdb-dataset/blob/main/imdb.zip?raw=true')\n",
        "\n",
        "with ZipFile(io.BytesIO(r.content), 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/Imdb-dataset/train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EDaEIsWXptS"
      },
      "source": [
        "r = requests.get('https://github.com/nvntcs18/imdb-dataset/blob/main/test.zip?raw=true')\n",
        "\n",
        "with ZipFile(io.BytesIO(r.content), 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/Imdb-dataset/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mDsEEPQTNqe",
        "outputId": "137ea56a-5957-4819-b96e-9dd913ade931",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -- coding: utf-8 --\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "import os\n",
        "\n",
        "nltk.download('stopwords')\n",
        "tokenizer=ToktokTokenizer()\n",
        "\n",
        "def review_to_words(text):\n",
        "    soup = BeautifulSoup(text,\"html.parser\")\n",
        "    text=soup.get_text()\n",
        "    text = re.sub('\\[[^]]*\\]', '', text)\n",
        "    text = re.sub(r\"[^a-zA-Z]\",\" \",text)\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    ps=PorterStemmer()\n",
        "    stemmed_tokens = [ps.stem(word) for word in tokens]\n",
        "    stop=set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in stemmed_tokens if token not in stop]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "def get_train_data():\n",
        "    filenames = []\n",
        "    for _,_,file in os.walk('/content/Imdb-dataset/train/imdb/pos'):\n",
        "        filenames = file\n",
        "    x_train=[]\n",
        "    y_train=[]\n",
        "    for filename in filenames:\n",
        "         with open('/content/Imdb-dataset/train/imdb/pos/'+filename, 'r') as f:\n",
        "             corpus = f.read()\n",
        "             x_train.append(corpus)\n",
        "             y_train.append(1)\n",
        "                    \n",
        "    for _,_,file in os.walk('/content/Imdb-dataset/train/imdb/neg'):\n",
        "        filenames = file\n",
        "    for filename in filenames:\n",
        "         with open('/content/Imdb-dataset/train/imdb/neg/'+filename, 'r') as f:\n",
        "             corpus = f.read()\n",
        "             x_train.append(corpus)\n",
        "             y_train.append(0)\n",
        "    return x_train,y_train\n",
        "\n",
        "def get_test_data():\n",
        "    filenames = []\n",
        "    for _,_,file in os.walk('/content/Imdb-dataset/test/test01/pos'):\n",
        "        filenames = file\n",
        "    x_train=[]\n",
        "    y_train=[]\n",
        "    for filename in filenames:\n",
        "         with open('/content/Imdb-dataset/test/test01/pos/'+filename, 'r') as f:\n",
        "             corpus = f.read()\n",
        "             x_train.append(corpus)\n",
        "             y_train.append(1)\n",
        "                    \n",
        "    for _,_,file in os.walk('/content/Imdb-dataset/test/test01/neg'):\n",
        "        filenames = file\n",
        "    for filename in filenames:\n",
        "         with open('/content/Imdb-dataset/test/test01/neg/'+filename, 'r') as f:\n",
        "             corpus = f.read()\n",
        "             x_train.append(corpus)\n",
        "             y_train.append(0)\n",
        "    return x_train,y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN29ZK6OdfUc",
        "outputId": "64112404-bb2a-493d-9bc1-426f1807edc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(review_to_words(get()[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwel high cartoon comedi it ran time program school life teacher my year teach profess lead believ bromwel high satir much closer realiti teacher scrambl surviv financi insight student see right pathet teacher pomp petti whole situat remind school i knew student i saw episod student repeatedli tri burn school i immedi recal high a classic line inspector i sack one teacher student welcom bromwel high i expect mani adult age think bromwel high far fetch piti\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLvgdn3knGmH"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from data_loader import get_data,review_to_words\n",
        "\n",
        "X_train,y_train = get_train_data()\n",
        "X_test,y_test = get_test_data()\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.fit_transform(y_test)\n",
        "\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    X_train[i] = review_to_words(X_train[i])\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    X_test[i] = review_to_words(X_test[i])\n",
        "    \n",
        "#X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "\n",
        "cv=CountVectorizer(binary=False,ngram_range=(1,3), max_features=50000)\n",
        "\n",
        "cv_train_reviews=cv.fit_transform(X_train)\n",
        "\n",
        "cv_test_reviews=cv.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COUb17dMgErT",
        "outputId": "4a12c527-314b-48c9-8200-9094d2dd5d38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "linear = svm.SVC(kernel='linear')\n",
        "linear_fit = linear.fit(cv_train_reviews, y_train)\n",
        "print(linear)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvZpefyvxhKU",
        "outputId": "b6fde7bd-0e12-4bc4-d68c-45e0f5203c06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "linear_predict = linear_fit.predict(cv_test_reviews)\n",
        "print(linear_predict)\n",
        "\n",
        "linear_score = accuracy_score(y_test,linear_predict)\n",
        "print(\"linear_score :\",linear_score)\n",
        "\n",
        "linear_report=classification_report(list(le.inverse_transform(y_test)),list(le.inverse_transform(linear_predict)),target_names=['pos','neg'])\n",
        "print(linear_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 0 0 0]\n",
            "linear_score : 0.86172\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         pos       0.86      0.87      0.86     12500\n",
            "         neg       0.87      0.86      0.86     12500\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jBuqrUuwmLH"
      },
      "source": [
        "linear_train_predict = linear_fit.predict(cv_train_reviews)\n",
        "print(linear_train_predict)\n",
        "\n",
        "linear_train_score = accuracy_score(y_train,linear_train_predict)\n",
        "print(\"linear_train_score :\",linear_train_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWBjYjzdFICX",
        "outputId": "12933ef0-555b-4138-b2ac-32db82baaa1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cm_bow=confusion_matrix(y_test,linear_predict,labels=[1,0])\n",
        "print(cm_bow)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[10704  1796]\n",
            " [ 1661 10839]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}